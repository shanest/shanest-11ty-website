---
references:
- id: banTestingPretrainedLanguage2022
  abstract: >-
    To what extent do pre-trained language models grasp semantic knowledge
    regarding the phenomenon of distributivity? In this paper, we introduce
    DistNLI, a new diagnostic dataset for natural language inference that
    targets the semantic difference arising from distributivity, and employ the
    causal mediation analysis framework to quantify the model behavior and
    explore the underlying mechanism in this semantically-related task. We find
    that the extent of models' understanding is associated with model size and
    vocabulary size. We also provide insights into how models encode such
    high-level semantic knowledge.
  accessed:
    - year: 2022
      month: 10
      day: 12
  author:
    - family: Ban
      given: Pangbo
    - family: Jiang
      given: Yifan
    - family: Liu
      given: Tianran
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: banTestingPretrainedLanguage2022
  container-title: >-
    Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting
    Neural Networks for NLP
  issued:
    - year: 2022
  title: >-
    Testing Pre-trained Language Models’ Understanding of Distributivity via
    Causal Mediation Analysis
  type: paper-conference
  URL: https://openreview.net/forum?id=D3U5fpVsZIN

- id: carcassiEmergenceMonotoneQuantifiers2019
  author:
    - family: Carcassi
      given: Fausto
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: carcassiEmergenceMonotoneQuantifiers2019
  container-title: >-
    Proceedings of the 41st Annual Meeting of the Cognitive Science Society
    (CogSci 2019)
  issued:
    - year: 2019
  page: 190-196
  title: The emergence of monotone quantifiers via iterated learning
  type: paper-conference
  URL: https://psyarxiv.com/8swtd

- id: carcassiMonotoneQuantifiersEmerge2021
  abstract: >-
    Natural languages exhibit many semantic universals, that is, properties of
    meaning shared across all languages. In this paper, we develop an
    explanation of one very prominent semantic universal, the monotonicity
    universal. While the existing work has shown that quantifiers satisfying the
    monotonicity universal are easier to learn, we provide a more complete
    explanation by considering the emergence of quantifiers from the perspective
    of cultural evolution. In particular, we show that quantifiers satisfy the
    monotonicity universal evolve reliably in an iterated learning paradigm with
    neural networks as agents.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Carcassi
      given: Fausto
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: carcassiMonotoneQuantifiersEmerge2021
  container-title: Cognitive Science
  DOI: 10.1111/cogs.13027
  ISSN: 1551-6709
  issue: '8'
  issued:
    - year: 2021
  language: en
  page: e13027
  source: Wiley Online Library
  title: Monotone Quantifiers Emerge via Iterated Learning
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13027
  volume: '45'

- id: carcassiMonotoneQuantifiersEmerge2021a
  abstract: >-
    Natural languages exhibit many semantic universals, that is, properties of
    meaning shared across all languages. In this paper, we develop an
    explanation of one very prominent semantic universal, the monotonicity
    universal. While the existing work has shown that quantifiers satisfying the
    monotonicity universal are easier to learn, we provide a more complete
    explanation by considering the emergence of quantifiers from the perspective
    of cultural evolution. In particular, we show that quantifiers satisfy the
    monotonicity universal evolve reliably in an iterated learning paradigm with
    neural networks as agents.
  accessed:
    - year: 2023
      month: 10
      day: 5
  author:
    - family: Carcassi
      given: Fausto
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: carcassiMonotoneQuantifiersEmerge2021a
  container-title: Cognitive Science
  DOI: 10.1111/cogs.13027
  ISSN: 1551-6709
  issue: '8'
  issued:
    - year: 2021
  language: en
  license: >-
    © 2021 The Authors. Cognitive Science published by Wiley Periodicals LLC on
    behalf of Cognitive Science Society (CSS).
  page: e13027
  source: Wiley Online Library
  title: Monotone Quantifiers Emerge via Iterated Learning
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13027
  volume: '45'

- id: denicComplexityInformativenessTradeoff2020
  abstract: >-
    The vocabulary of human languages has been argued to support efficient
    communication by optimizing the trade-off between complexity and
    informativeness (Kemp & Regier 2012). The argument has been based on
    cross-linguistic analyses of vocabulary in semantic domains of content words
    such as kinship, color, and number terms. The present work extends this
    analysis to a category of function words: indefinite pronouns (e.g. someone,
    anyone, no-one, cf. Haspelmath 2001). We build on previous work to establish
    the meaning space and featural make-up for indefinite pronouns, and show
    that indefinite pronoun systems across languages optimize the
    complexity/informativeness trade-off. This demonstrates that pressures for
    efficient communication shape both content and function word categories,
    thus tying in with the conclusions of recent work on quantifiers by
    Steinert-Threlkeld (2019). Furthermore, we argue that the trade-off may
    explain some of the universal properties of indefinite pronouns, thus
    reducing the explanatory load for linguistic theories.
  author:
    - family: Denić
      given: Milica
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: denicComplexityInformativenessTradeoff2020
  container-title: Proceedings of Semantics and Linguistic Theory (SALT 30)
  DOI: 10.3765/salt.v30i0.4811
  issued:
    - year: 2020
  page: 166-184
  title: Complexity/informativeness trade-off in the domain of indefinite pronouns
  type: paper-conference
  URL: >-
    https://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/30.166
  volume: '30'

- id: denicIndefinitePronounsOptimize2022
  abstract: >-
    The vocabulary of human languages has been argued to support efficient
    communication by optimizing the trade-off between simplicity and
    informativeness. The argument has been originally based on cross-linguistic
    analyses of vocabulary in semantic domains of content words, such as
    kinship, color, and number terms. The present work applies this analysis to
    a category of function words: indefinite pronouns (e.g., someone, anyone, no
    one). We build on previous work to establish the meaning space and featural
    make-up for indefinite pronouns, and show that indefinite pronoun systems
    across languages optimize the simplicity/informativeness trade-off. This
    demonstrates that pressures for efficient communication shape both content
    and function word categories. In doing so, our work aligns with several
    concurrent studies exploring the simplicity/informativeness trade-off in
    functional vocabulary. Importantly, we further argue that the trade-off may
    explain some of the universal properties of indefinite pronouns, thus
    reducing the explanatory load for linguistic theories.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Denić
      given: Milica
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: denicIndefinitePronounsOptimize2022
  container-title: Cognitive Science
  DOI: 10.1111/cogs.13142
  ISSN: 1551-6709
  issue: '5'
  issued:
    - year: 2022
  language: en
  page: e13142
  source: Wiley Online Library
  title: Indefinite Pronouns Optimize the Simplicity/Informativeness Trade-Off
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13142
  volume: '46'

- id: downeyEmbeddingStructureMatters2023
  accessed:
    - year: 2023
      month: 12
      day: 5
  author:
    - family: Downey
      given: C.m.
    - family: Blevins
      given: Terra
    - family: Goldfine
      given: Nora
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: downeyEmbeddingStructureMatters2023
  container-title: >-
    Proceedings of the 3rd Workshop on Multi-lingual Representation Learning
    (MRL)
  editor:
    - family: Ataman
      given: Duygu
  event-place: Singapore
  issued:
    - year: 2023
      month: 12
  page: 268–281
  publisher: Association for Computational Linguistics
  publisher-place: Singapore
  source: ACLWeb
  title: >-
    Embedding Structure Matters: Comparing Methods to Adapt Multilingual
    Vocabularies to New Languages
  title-short: Embedding Structure Matters
  type: paper-conference
  URL: https://aclanthology.org/2023.mrl-1.20

- id: downeyLearningTranslateLearning2023
  accessed:
    - year: 2023
      month: 12
      day: 5
  author:
    - family: Downey
      given: C.m.
    - family: Zhou
      given: Xuhui
    - family: Liu
      given: Zeyu
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: downeyLearningTranslateLearning2023
  container-title: >-
    Proceedings of the 3rd Workshop on Multi-lingual Representation Learning
    (MRL)
  editor:
    - family: Ataman
      given: Duygu
  event-place: Singapore
  issued:
    - year: 2023
      month: 12
  page: 218–238
  publisher: Association for Computational Linguistics
  publisher-place: Singapore
  source: ACLWeb
  title: Learning to translate by learning to communicate
  type: paper-conference
  URL: https://aclanthology.org/2023.mrl-1.17

- id: downeyLearningTranslateLearningsubmitted
  abstract: >-
    We formulate and test a technique to use Emergent Communication (EC) with a
    pretrained multilingual model to improve on modern Unsupervised NMT systems,
    especially for low-resource languages. It has been argued that the currently
    dominant paradigm in NLP of pretraining on text-only corpora will not yield
    robust natural language understanding systems, and the need for grounded,
    goal-oriented, and interactive language learning has been highlighted. In
    our approach, we embed a modern multilingual model (mBART, Liu et. al. 2020)
    into an EC image-reference game, in which the model is incentivized to use
    multilingual generations to accomplish a vision-grounded task, with the
    hypothesis that this will align multiple languages to a shared task space.
    We present two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022),
    one of which outperforms a backtranslation-based baseline in 6/8 translation
    settings, and proves especially beneficial for the very low-resource
    languages of Nepali and Sinhala.
  author:
    - family: Downey
      given: C. M.
    - family: Liu
      given: Leo Z.
    - family: Zhou
      given: Xuhui
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: downeyLearningTranslateLearningsubmitted
  container-title: Representation Learning for NLP (RepL4NLP)
  DOI: 10.48550/arXiv.2207.07025
  issued:
    - literal: submitted
  source: arXiv.org
  title: Learning to translate by learning to communicate
  type: paper-conference
  URL: http://arxiv.org/abs/2207.07025

- id: downeyMaskedSegmentalLanguage2022
  abstract: >-
    We introduce a Masked Segmental Language Model (MSLM) for joint language
    modeling and unsupervised segmentation. While near-perfect supervised
    methods have been developed for segmenting human-like linguistic units in
    resource-rich languages such as Chinese, many of the world's languages are
    both morphologically complex, and have no large dataset of “gold”
    segmentations for supervised training. Segmental Language Models offer a
    unique approach by conducting unsupervised segmentation as the byproduct of
    a neural language modeling objective. However, current SLMs are limited in
    their scalability due to their recurrent architecture. We propose a new type
    of SLM for use in both unsupervised and lightly supervised segmentation
    tasks. The MSLM is built on a span-masking transformer architecture,
    harnessing a masked bidirectional modeling context and attention, as well as
    adding the potential for model scalability. In a series of experiments, our
    model outperforms the segmentation quality of recurrent SLMs on Chinese, and
    performs similarly to the recurrent model on English.
  accessed:
    - year: 2022
      month: 7
      day: 11
  author:
    - family: Downey
      given: C.m.
    - family: Xia
      given: Fei
    - family: Levow
      given: Gina-Anne
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: downeyMaskedSegmentalLanguage2022
  container-title: >-
    Proceedings of the 19th SIGMORPHON Workshop on Computational Research in
    Phonetics, Phonology, and Morphology
  event-place: Seattle, Washington
  event-title: NAACL-SIGMORPHON 2022
  issued:
    - year: 2022
      month: 7
  page: 39–50
  publisher: Association for Computational Linguistics
  publisher-place: Seattle, Washington
  source: ACLWeb
  title: >-
    A Masked Segmental Language Model for Unsupervised Natural Language
    Segmentation
  type: paper-conference
  URL: https://aclanthology.org/2022.sigmorphon-1.5

- id: downeyTargetedMultilingualAdaptation2024
  abstract: >-
    Massively multilingual models are known to have limited utility in any one
    language, and to perform particularly poorly on low-resource languages. By
    contrast, targeted multinguality has been shown to benefit low-resource
    languages. To test this approach more rigorously, we systematically study
    best practices for adapting a pre-trained model to a language family.
    Focusing on the Uralic family as a test case, we adapt XLM-R under various
    configurations to model 15 languages; we then evaluate the performance of
    each experimental setting on two downstream tasks and 11 evaluation
    languages. Our adapted models significantly outperform mono- and
    multilingual baselines. A regression analysis reveals that adapted
    vocabulary size is relatively unimportant for low-resource languages, and
    that low-resource languages can be aggressively up-sampled during training
    at little detriment to performance in high-resource languages. These results
    introduce new best practices for performing language adaptation in a
    targeted setting.
  accessed:
    - year: 2024
      month: 11
      day: 12
  author:
    - family: Downey
      given: C. M.
    - family: Blevins
      given: Terra
    - family: Serai
      given: Dhwani
    - family: Parikh
      given: Dwija
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: downeyTargetedMultilingualAdaptation2024
  container-title: 'Findings of the Association for Computational Linguistics: EMNLP 2024'
  editor:
    - family: Al-Onaizan
      given: Yaser
    - family: Bansal
      given: Mohit
    - family: Chen
      given: Yun-Nung
  event-place: Miami, Florida, USA
  event-title: Findings 2024
  issued:
    - year: 2024
      month: 11
  page: 15647–15663
  publisher: Association for Computational Linguistics
  publisher-place: Miami, Florida, USA
  source: ACLWeb
  title: Targeted Multilingual Adaptation for Low-resource Language Families
  type: paper-conference
  URL: https://aclanthology.org/2024.findings-emnlp.918

- id: geffenlanSpontaneousEmergenceDiscrete2020
  abstract: >-
    We propose a general framework to study language emergence through signaling
    games with neural agents. Using a continuous latent space, we are able to
    (i) train using backpropagation, (ii) show that discrete messages
    nonetheless naturally emerge. We explore whether categorical perception
    effects follow and show that the messages are not compositional.
  author:
    - family: Geffen Lan
      given: Nur
    - family: Chemla
      given: Emmanuel
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: geffenlanSpontaneousEmergenceDiscrete2020
  container-title: >-
    Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics
  DOI: 10.18653/v1/2020.acl-main.433
  event-place: Stroudsburg, PA, USA
  issued:
    - year: 2020
  page: 4794-4800
  publisher: Association for Computational Linguistics
  publisher-place: Stroudsburg, PA, USA
  title: On the Spontaneous Emergence of Discrete and Compositional Signals
  type: paper-conference
  URL: https://www.aclweb.org/anthology/2020.acl-main.433

- id: guerinImpactSyntacticSemantic2024
  abstract: >-
    Unsupervised on-the-fly back-translation, in conjunction with multilingual
    pretraining, is the dominant method for unsupervised neural machine
    translation. Theoretically, however, the method should not work in general.
    We therefore conduct controlled experiments with artificial languages to
    determine what properties of languages make back-translation an effective
    training method, covering lexical, syntactic, and semantic properties. We
    find, contrary to popular belief, that (i)~parallel word frequency
    distributions, (ii)~partially shared vocabulary, and (iii)~similar syntactic
    structure across languages are not sufficient to explain the success of
    back-translation. We show however that even crude semantic signal (similar
    lexical fields across languages) does improve alignment of two languages
    through back-translation. We conjecture that rich semantic dependencies,
    parallel across languages, are at the root of the success of unsupervised
    methods based on back-translation. Overall, the success of unsupervised
    machine translation was far from being analytically guaranteed. Instead, it
    is another proof that languages of the world share deep similarities, and we
    hope to show how to identify which of these similarities can serve the
    development of unsupervised, cross-linguistic tools.
  accessed:
    - year: 2024
      month: 11
      day: 12
  author:
    - family: Guerin
      given: Nicolas
    - family: Chemla
      given: Emmanuel
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: guerinImpactSyntacticSemantic2024
  container-title: Transactions on Machine Learning Research
  ISSN: 2835-8856
  issued:
    - year: 2024
  language: en
  source: openreview.net
  title: >-
    The Impact of Syntactic and Semantic Proximity on Machine Translation with
    Back-Translation
  type: article-journal
  URL: https://openreview.net/forum?id=6DflIABPQP

- id: guerinSecondorderZipfsLaw2025
  author:
    - family: Guerin
      given: Nicolas
    - family: Steinert-Threlkeld
      given: Shane
    - family: Ryder
      given: Robin
    - family: Chemla
      given: Emmanuel
  citation-key: guerinSecondorderZipfsLaw2025
  container-title: Open Mind
  DOI: 10.1162/opmi.a.8
  issued:
    - year: 2025
  title: Second-order Zipf's Law for Word  Co-occurrences
  type: article-journal

- id: guoDatabaseModalSemantic2022
  abstract: >-
    This paper introduces a database for crosslinguistic modal semantics. The
    purpose of this database is to (1) enable ongoing consolidation of modal
    semantic typological knowledge into a repository according to uniform data
    standards and to (2) provide data for investigations in crosslinguistic
    modal semantic theory and experiments explaining such theories. We describe
    the kind of semantic variation that the database aims to record, the format
    of the data, and a current snapshot of the database, emphasizing access and
    contribution to the database in light of the goals above. We release the
    database at https://clmbr.shane.st/modal-typology.
  accessed:
    - year: 2022
      month: 7
      day: 11
  author:
    - family: Guo
      given: Qingxia
    - family: Imel
      given: Nathaniel
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: guoDatabaseModalSemantic2022
  container-title: >-
    Proceedings of the 4th Workshop on Research in Computational Linguistic
    Typology and Multilingual NLP
  DOI: 10.18653/v1/2022.sigtyp-1.6
  event-place: Seattle, Washington
  event-title: NAACL-SIGTYP 2022
  issued:
    - year: 2022
      month: 7
  page: 42–51
  publisher: Association for Computational Linguistics
  publisher-place: Seattle, Washington
  source: ACLWeb
  title: A Database for Modal Semantic Typology
  type: paper-conference
  URL: https://aclanthology.org/2022.sigtyp-1.6

- id: haberlandQuantifiersThatAre2025
  author:
    - family: Haberland
      given: Christopher
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: haberlandQuantifiersThatAre2025
  container-title: Proceedings of Semantics and Linguistic Theory (SALT 35)
  issued:
    - year: 2025
  title: Quantifiers that are More Monotone are Easier to Learn
  type: paper-conference

- id: Hawke2015
  author:
    - family: Hawke
      given: Peter
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: Hawke2015
  container-title: Proceedings of Logic, Rationality, and Interaction (LORI-V)
  DOI: 10.1007/978-3-662-48561-3_12
  editor:
    - family: Hoek
      given: Wiebe
      dropping-particle: van der
    - family: Holliday
      given: Wesley H
    - family: Wang
      given: Wen-fang
  issued:
    - year: 2015
  page: 143-155
  title: Informational Dynamics of `Might' Assertions
  type: paper-conference
  volume: '9394'

- id: hawkeInformationalDynamicsEpistemic2018
  abstract: >-
    We investigate, in a logical setting, the expressivist proposal that
    assertion primarily functions to express and coordinate doxastic states and
    that ‘might’ fundamentally expresses lack of belief. We provide a formal
    model of an agent’s doxastic state and novel assertability conditions for an
    associated formal language. We thereby prove that an arbitrary assertion
    (including a complex of ‘might’ and ‘factual’ claims) always succeeds in
    expressing a well-defined (partial) doxastic state, and propose a fully
    general and intuitive update operation as a model of an agent coming to
    accept an arbitrary assertion. Leaving a comprehensive philosophical and
    linguistic defense for elsewhere, we explore technical aspects of our
    framework, providing, for instance, a complete logic of assertability and
    reduction axioms for the novel update operations related to our proposal.
    Finally, we contrast our work with related proposals in the logic
    literature.
  accessed:
    - year: 2022
      month: 3
      day: 21
  author:
    - family: Hawke
      given: Peter
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: hawkeInformationalDynamicsEpistemic2018
  container-title: Synthese
  container-title-short: Synthese
  DOI: 10.1007/s11229-016-1216-8
  ISSN: 1573-0964
  issue: '10'
  issued:
    - year: 2018
  language: en
  page: 4309-4342
  source: Springer Link
  title: Informational dynamics of epistemic possibility modals
  type: article-journal
  URL: https://doi.org/10.1007/s11229-016-1216-8
  volume: '195'

- id: hawkeInformationalDynamicsMight2015
  abstract: >-
    We investigate, in a logical setting, the proposal that assertion primarily
    functions to express and coordinate doxastic states and that ‘might’
    fundamentally expresses lack of belief. We provide a formal model of an
    agent’s doxastic state and precise assertability conditions for an
    associated formal language. We thereby prove that an arbitrary assertion
    (including a complex of ‘might’ and ‘factual’ claims) always succeeds in
    expressing a well-defined doxastic state. We then propose a fully general
    and intuitive doxastic update operation as a model of an agent coming to
    accept an arbitrary assertion. We provide reduction axioms for some novel
    update operations related to this proposal.
  author:
    - family: Hawke
      given: Peter
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: hawkeInformationalDynamicsMight2015
  collection-title: Lecture Notes in Computer Science
  container-title: Logic, Rationality, and Interaction
  DOI: 10.1007/978-3-662-48561-3_12
  editor:
    - family: Hoek
      given: Wiebe
      non-dropping-particle: van der
    - family: Holliday
      given: Wesley H.
    - family: Wang
      given: Wen-fang
  event-place: Berlin, Heidelberg
  ISBN: 978-3-662-48561-3
  issued:
    - year: 2015
  language: en
  page: 143-155
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: Informational Dynamics of ‘Might’ Assertions
  type: paper-conference

- id: hawkeSemanticExpressivismEpistemic2021
  abstract: >-
    Expressivists about epistemic modals deny that ‘Jane might be late’
    canonically serves to express the speaker’s acceptance of a certain
    propositional content. Instead, they hold that it expresses a lack of
    acceptance (that Jane isn’t late). Prominent expressivists embrace pragmatic
    expressivism: the doxastic property expressed by a declarative is not
    helpfully identified with (any part of) that sentence’s compositional
    semantic value. Against this, we defend semantic expressivism about
    epistemic modals: the semantic value of a declarative from this domain is
    (partly) the property of doxastic attitudes it canonically serves to
    express. In support, we synthesize data from the critical literature on
    expressivism—largely reflecting interactions between modals and
    disjunctions—and present a semantic expressivism that readily predicts the
    data. This contrasts with salient competitors, including: pragmatic
    expressivism based on domain semantics or dynamic semantics; semantic
    expressivism à la Moss (Semant Pragmat 8(5):1–81, 2015.
    https://doi.org/10.3765/sp.8.5); and the bounded relational semantics of
    Mandelkern (Philos Rev 128(1):1–61, 2019.
    https://doi.org/10.1215/00318108-7213001).
  accessed:
    - year: 2022
      month: 3
      day: 21
  author:
    - family: Hawke
      given: Peter
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: hawkeSemanticExpressivismEpistemic2021
  container-title: Linguistics and Philosophy
  container-title-short: Linguist and Philos
  DOI: 10.1007/s10988-020-09295-7
  ISSN: 1573-0549
  issue: '2'
  issued:
    - year: 2021
  language: en
  page: 475-511
  source: Springer Link
  title: Semantic expressivism for epistemic modals
  type: article-journal
  URL: https://doi.org/10.1007/s10988-020-09295-7
  volume: '44'

- id: imelModalSemanticUniversals2022
  abstract: "The\_meanings\_expressed\_by\_the\_world’s\_languages\_have\_been\_argued\_to support efficient communication.\_Evidence for this hypothesis has drawn on\_cross-linguistic analyses of vocabulary in semantic domains of both content words\_(e.g. kinship terms (Kemp & Regier 2012); color terms (Regier, Kay & Khetarpal\_2007; Zaslavsky, Kemp, Regier & Tishby 2018)) and function words (e.g.quantifiers(Steinert-Threlkeld2021);\_indefinite\_pronouns(Deni ́c,\_Steinert-Threlkeld\_& Szymanik 2022)) approaching the hypothesis concretely in terms of a trade-off\_between simplicity and informativeness. We apply the analysis to\_modals\_(e.g.\_can,\_ought,\_might).\_Two proposed universals in this domain from Nauze (2008) and\_Vander Klok (2013) are used for generating many artificial languages with varying\_degrees of quasi-naturalness as a proxy for natural data. A computational experiment\_shows that most of the optimal solutions to the trade-off problem are predicted by\_Vander Klok; meanwhile, as languages more robustly satisfy Nauze’s universal, they\_also become more optimal. This suggests that efficient communication is a leading\_explanation for constraints on modal semantic variation."
  accessed:
    - year: 2022
      month: 12
      day: 29
  author:
    - family: Imel
      given: Nathaniel
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: imelModalSemanticUniversals2022
  container-title: Semantics and Linguistic Theory (SALT 32)
  DOI: 10.3765/salt.v1i0.5346
  issued:
    - year: 2022
  language: en
  license: Copyright (c) 2022 Nathaniel Imel
  number: '0'
  page: 227-248
  source: journals.linguisticsociety.org
  title: Modal semantic universals optimize the simplicity/informativeness trade-off
  type: paper-conference
  URL: >-
    http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/32.012

- id: imelUnnaturalLanguageToolKit2025
  abstract: "The Unnatural Language Toolkit (ULTK) is an open-source Python library for computational semantic typology research (https://clmbr.shane.st/ultk/ultk.html). ULTK's key features include unifying data structures, algorithms for generating artificial languages, and data analysis tools for related computational experiments. The\_language module organizes the basic data structures for constructing meaning spaces, expressions, and languages. A\_grammar submodule contains methods for building and enumerating expressions from custom Language of Thought (Fodor, 1975, 2008; Quilty-Dunn et al., 2022) grammars, which allows for straightforward computation of minimum length descriptions for symbolically expressible semantic representations. This approach has been used successfully in many investigations of concept learning (Feldman, 2000; Goodman et al., 2015). The second main module of ULTK, effcomm, organizes efficient communication analyses, which have become popular styles of explanation in recent functionalist accounts of semantic universals (Kemp et al., 2018). This module contains functions for defining informativity based on literal and pragmatic communicative agents and algorithms for exploring the space of artificial languages. After first elaborating on the structure of these two modules, we then provide two case studies, illustrating two major styles of explanation in computational semantic typology research: (1) an efficient communication analysis of modal semantic typology, and (2) an analysis of the relative ease of learning of monotone versus non-monotone quantifiers. ULTK’s accessible design, documentation, and open-source nature are intended to reduce barriers for researchers when implementing computational linguistic typological experiments."
  accessed:
    - year: 2025
      month: 8
      day: 8
  author:
    - family: Imel
      given: Nathaniel
    - family: Haberland
      given: Christopher
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: imelUnnaturalLanguageToolKit2025
  container-title: Society for Computation in Linguistics
  DOI: 10.7275/scil.3144
  ISSN: 2834-1007
  issue: '1'
  issued:
    - year: 2025
  language: eng
  number: '1'
  publisher: University of Massachusetts Amherst Libraries
  source: openpublishing.library.umass.edu
  title: The Unnatural Language ToolKit (ULTK)
  type: article-journal
  URL: https://openpublishing.library.umass.edu/scil/article/id/3144/
  volume: '8'

- id: jiangWeightedMobiusScoresubmitted
  abstract: >-
    Feature attribution aims to explain the reasoning behind a black-box model's
    prediction by identifying the impact of each feature on the prediction.
    Recent work has extended feature attribution to interactions between
    multiple features. However, the lack of a unified framework has led to a
    proliferation of methods that are often not directly comparable. This paper
    introduces a parameterized attribution framework -- the Weighted M\"obius
    Score -- and (i) shows that many different attribution methods for both
    individual features and feature interactions are special cases and (ii)
    identifies some new methods. By studying the vector space of attribution
    methods, our framework utilizes standard linear algebra tools and provides
    interpretations in various fields, including cooperative game theory and
    causal mediation analysis. We empirically demonstrate the framework's
    versatility and effectiveness by applying these attribution methods to
    feature interactions in sentiment analysis and chain-of-thought prompting.
  author:
    - family: Jiang
      given: Yifan
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: jiangWeightedMobiusScoresubmitted
  container-title: Neural Information Processing Systems
  DOI: 10.48550/arXiv.2305.09204
  issued:
    - literal: submitted
  source: arXiv.org
  title: 'The Weighted Möbius Score: A Unified Framework for Feature Attribution'
  title-short: The Weighted Möbius Score
  type: paper-conference
  URL: http://arxiv.org/abs/2305.09204

- id: jumeletLanguageModelsUse2021
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Jumelet
      given: Jaap
    - family: Denic
      given: Milica
    - family: Szymanik
      given: Jakub
    - family: Hupkes
      given: Dieuwke
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: jumeletLanguageModelsUse2021
  container-title: 'Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021'
  DOI: 10.18653/v1/2021.findings-acl.439
  event-place: Online
  event-title: Findings 2021
  issued:
    - year: 2021
      month: 8
  page: 4958–4969
  publisher: Association for Computational Linguistics
  publisher-place: Online
  source: ACLWeb
  title: Language Models Use Monotonicity to Assess NPI Licensing
  type: paper-conference
  URL: https://aclanthology.org/2021.findings-acl.439

- id: liLinguisticallyInformedTransformationsLIT2020
  abstract: >-
    Although large-scale pretrained language models, such as BERT and RoBERTa,
    have achieved superhuman performance on in-distribution test sets, their
    performance suffers on out-of-distribution test sets (e.g., on contrast
    sets). Building contrast sets often requires human-expert annotation, which
    is expensive and hard to create on a large scale. In this work, we propose a
    Linguistically-Informed Transformation (LIT) method to automatically
    generate contrast sets, which enables practitioners to explore linguistic
    phenomena of interests as well as compose different phenomena. Experimenting
    with our method on SNLI and MNLI shows that current pretrained language
    models, although being claimed to contain sufficient linguistic knowledge,
    struggle on our automatically generated contrast sets. Furthermore, we
    improve models' performance on the contrast sets by applying LIT to augment
    the training data, without affecting performance on the original data.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Li
      given: Chuanrong
    - family: Shengshuo
      given: Lin
    - family: Liu
      given: Zeyu
    - family: Wu
      given: Xinyi
    - family: Zhou
      given: Xuhui
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: liLinguisticallyInformedTransformationsLIT2020
  container-title: >-
    Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting
    Neural Networks for NLP
  DOI: 10.18653/v1/2020.blackboxnlp-1.12
  event-place: Online
  event-title: BlackboxNLP-EMNLP 2020
  issued:
    - year: 2020
      month: 11
  page: 126–135
  publisher: Association for Computational Linguistics
  publisher-place: Online
  source: ACLWeb
  title: >-
    Linguistically-Informed Transformations (LIT): A Method for Automatically
    Generating Contrast Sets
  title-short: Linguistically-Informed Transformations (LIT)
  type: paper-conference
  URL: https://aclanthology.org/2020.blackboxnlp-1.12

- id: mintsADCMethodProof2016
  author:
    - family: Mints
      given: Grigori
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: mintsADCMethodProof2016
  container-title: Journal of Logic and Computation
  DOI: 10.1093/logcom/ext032
  ISSN: 0955-792X
  issue: '1'
  issued:
    - year: 2016
  page: 395-408
  title: ADC Method of Proof Search in Intuitionistic Propositional Natural Deduction
  type: article-journal
  volume: '26'

- id: ogiharaLimitationsModalAnalysis2024
  abstract: >-
    This article takes a critical view of Beaver &amp; Condoravdi’s (2003) modal
    analysis of before and after. According to their proposal, the clause headed
    by before or after denotes the earliest possible time at which it is true.
    We first show that the original proposal presented by Beaver &amp;
    Condoravdi (2003) faces difficulty with anti-veridical before-clause cases.
    We then incorporate eventualities (events and states) into a revamped
    proposal in which the existence of an eventuality that could lead to a
    before-clause eventuality and that parallels a very similar eventuality in
    the actual world is used as a criterion for selecting the set of alternative
    worlds. This allows the alternative worlds to differ from the actual one at
    a time earlier than the matrix clause predication time. However, this
    revision still suffers from counterexamples that involve before clauses that
    refer back to a time before the matrix clause eventuality. This discussion
    leaves room for the possibility that an extensional account might offer a
    better analysis.

    EARLY ACCESS
  accessed:
    - year: 2024
      month: 1
      day: 8
  author:
    - family: Ogihara
      given: Toshiyuki
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: ogiharaLimitationsModalAnalysis2024
  container-title: Semantics and Pragmatics
  DOI: 10.3765/sp.17.1
  ISSN: 1937-8912
  issue: '1'
  issued:
    - year: 2024
  language: en
  license: Copyright (c) 2024 Toshiyuki Ogihara, Shane Steinert-Threlkeld
  source: semprag.org
  title: Limitations of a modal analysis of *before* and *after*
  type: article-journal
  URL: https://semprag.org/index.php/sp/article/view/sp.17.1
  volume: '17'

- id: osullivanNeuralModelsPsychosemantics2019
  abstract: >-
    How are the meanings of linguistic expressions related to their use in
    concrete cognitive tasks? Visual identification tasks show human speakers
    can exhibit considerable variation in their understanding, representation
    and verification of certain quantifiers. This paper initiates an
    investigation into neural models of these psycho-semantic tasks. We trained
    two types of network -- a convolutional neural network (CNN) model and a
    recurrent model of visual attention (RAM) -- on the "most" verification task
    from \citet{Pietroski2009}, manipulating the visual scene and novel notions
    of task duration. Our results qualitatively mirror certain features of human
    performance (such as sensitivity to the ratio of set sizes, indicating a
    reliance on approximate number) while differing in interesting ways (such as
    exhibiting a subtly different pattern for the effect of image type). We
    conclude by discussing the prospects for using neural models as cognitive
    models of this and other psychosemantic tasks.
  author:
    - family: O’Sullivan
      given: Lewis
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: osullivanNeuralModelsPsychosemantics2019
  container-title: >-
    Proceedings of the Workshop on Cognitive Modeling and Computational
    Linguistics
  DOI: 10.18653/v1/W19-2916
  event-place: Stroudsburg, PA, USA
  issue: '2009'
  issued:
    - year: 2019
  page: 140-151
  publisher: Association for Computational Linguistics
  publisher-place: Stroudsburg, PA, USA
  title: Neural Models of the Psychosemantics of ‘Most’
  type: paper-conference
  URL: http://aclweb.org/anthology/W19-2916

- id: patilFilteredCorpusTraining2024a
  abstract: >-
    This paper introduces Filtered Corpus Training, a method that trains
    language models (LMs) on corpora with certain linguistic constructions
    filtered out from the training data, and uses it to measure the ability of
    LMs to perform linguistic generalization on the basis of indirect evidence.
    We apply the method to both LSTM and Transformer LMs (of roughly comparable
    size), developing filtered corpora that target a wide range of linguistic
    phenomena. Our results show that while transformers are better qua LMs (as
    measured by perplexity), both models perform equally and surprisingly well
    on linguistic generalization measures, suggesting that they are capable of
    generalizing from indirect evidence.
  accessed:
    - year: 2025
      month: 8
      day: 8
  author:
    - family: Patil
      given: Abhinav
    - family: Jumelet
      given: Jaap
    - family: Chiu
      given: Yu Ying
    - family: Lapastora
      given: Andy
    - family: Shen
      given: Peter
    - family: Wang
      given: Lexie
    - family: Willrich
      given: Clevis
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: patilFilteredCorpusTraining2024a
  container-title: Transactions of the Association for Computational Linguistics
  container-title-short: Transactions of the Association for Computational Linguistics
  DOI: 10.1162/tacl_a_00720
  ISSN: 2307-387X
  issued:
    - year: 2024
  page: 1597-1615
  source: Silverchair
  title: >-
    Filtered Corpus Training (FiCT) Shows that Language Models Can Generalize
    from Indirect Evidence
  type: article-journal
  URL: https://doi.org/10.1162/tacl_a_00720
  volume: '12'

- id: Pezzelle2018
  author:
    - family: Pezzelle
      given: Sandro
    - family: Steinert-Threlkeld
      given: Shane
    - family: Bernardi
      given: Raffaella
    - family: Szymanik
      given: Jakub
  citation-key: Pezzelle2018
  container-title: >-
    Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (ACL 2018)
  issued:
    - year: 2018
  title: >-
    Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in
    Predicting Quantifiers
  type: paper-conference

- id: pezzelleThemCanBe2018
  abstract: >-
    We study the role of linguistic context in predicting quantifiers (`few',
    `all'). We collect crowdsourced data from human participants and test
    various models in a local (single-sentence) and a global context
    (multi-sentence) condition. Models significantly out-perform humans in the
    former setting and are only slightly better in the latter. While human
    performance improves with more linguistic context (especially on
    proportional quantifiers), model performance suffers. Models are very
    effective in exploiting lexical and morpho-syntactic patterns; humans are
    better at genuinely understanding the meaning of the (global) context.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Pezzelle
      given: Sandro
    - family: Steinert-Threlkeld
      given: Shane
    - family: Bernardi
      given: Raffaella
    - family: Szymanik
      given: Jakub
  citation-key: pezzelleThemCanBe2018
  container-title: >-
    Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)
  DOI: 10.18653/v1/P18-2019
  event-place: Melbourne, Australia
  event-title: ACL 2018
  issued:
    - year: 2018
      month: 7
  page: 114–119
  publisher: Association for Computational Linguistics
  publisher-place: Melbourne, Australia
  source: ACLWeb
  title: >-
    Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in
    Predicting Quantifiers
  type: paper-conference
  URL: https://aclanthology.org/P18-2019

- id: Pol2021
  author:
    - family: Pol
      given: Iris
      dropping-particle: van de
    - family: Lodder
      given: Paul
    - family: Maanen
      given: Leendert
      dropping-particle: van
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: Pol2021
  container-title: Proceedings of the Annual Meeting of the Cognitive Science Society
  issued:
    - year: 2021
  title: Quantifiers satisfying semantic universals are simpler
  type: paper-conference
  URL: https://escholarship.org/uc/item/1vm445rp
  volume: '43'

- id: polComplexityLearnabilityExplanation2019
  author:
    - family: Pol
      given: Iris
      dropping-particle: van de
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: polComplexityLearnabilityExplanation2019
  container-title: >-
    Proceedings of the 41st Annual Meeting of the Cognitive Science Society
    (CogSci 2019)
  issued:
    - year: 2019
  page: 3015-3021
  title: >-
    Complexity and learnability in the explanation of semantic universals of
    quantifiers
  type: paper-conference
  URL: https://psyarxiv.com/f8dbp/

- id: polQuantifiersSatisfyingSemantic2023
  author:
    - family: Pol
      given: Iris
      dropping-particle: van de
    - family: Lodder
      given: Paul
    - family: Maanen
      given: Leendert
      dropping-particle: van
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: polQuantifiersSatisfyingSemantic2023
  container-title: Cognition
  DOI: 10.1016/j.cognition.2022.105150
  issued:
    - year: 2023
  page: '105150'
  title: >-
    Quantifiers satisfying semantic universals have shorter minimal description
    length
  type: article-journal
  volume: '232'

- id: ramotowskaMostNotMore2020
  abstract: >-
    In this study we test individual differences in the meaning representations
    of two natural language quantifiers – most and more than half – in a novel,
    purely linguistic task. We operationalized differences in meaning
    representations as differences in individual thresholds which were estimated
    using logistic regression. We show that the representation ofmost varies
    across subjects and its verification depends on proportion. Moreover, the
    choice of the representation of most affects the verification process. These
    effects are not present for more than half. The study demonstrates the
    cognitive differences between most and more than half and individual
    variation in meaning representations.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Ramotowska
      given: Sonia
    - family: Steinert-Threlkeld
      given: Shane
    - family: Maanen
      given: Leendert Van
    - family: Szymanik
      given: Jakub
  citation-key: ramotowskaMostNotMore2020
  container-title: Proceedings of Sinn und Bedeutung
  DOI: 10.18148/sub/2020.v24i2.891
  ISSN: 2629-6055
  issue: '2'
  issued:
    - year: 2020
  language: en
  license: >-
    Copyright (c) 2020 Sonia Ramotowska, Shane Steinert-Threlkeld, Leendert Van
    Maanen, Jakub Szymanik
  number: '2'
  page: 165-182
  source: ojs.ub.uni-konstanz.de
  title: >-
    Most, but not more than half, is proportion-dependent and sensitive to
    individual differences
  type: article-journal
  URL: https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/891
  volume: '24'

- id: ramotowskaUncoveringStructureSemantic2023
  abstract: >-
    According to logical theories of meaning, a meaning of an expression can be
    formalized and encoded in truth conditions. Vagueness of the language and
    individual differences between people are a challenge to incorporate into
    the meaning representations. In this paper, we propose a new approach to
    study truth-conditional representations of vague concepts. For a case study,
    we selected two natural language quantifiers most and more than half. We
    conducted two online experiments, each with 90 native English speakers. In
    the first experiment, we tested between-subjects variability in meaning
    representations. In the second experiment, we tested the stability of
    meaning representations over time by testing the same group of participants
    in two experimental sessions. In both experiments, participants performed
    the verification task. They verified a sentence with a quantifier (e.g.,
    “Most of the gleerbs are feezda.”) based on the numerical information
    provided in the second sentence, (e.g., “60% of the gleerbs are feezda”). To
    investigate between-subject and within-subject differences in meaning
    representations, we proposed an extended version of the Diffusion Decision
    Model with two parameters capturing truth conditions and vagueness. We fit
    the model to responses and reaction times data. In the first experiment, we
    found substantial between-subject differences in representations of most as
    reflected by the variability in the truth conditions. Moreover, we found
    that the verification of most is proportion-dependent as reflected in the
    reaction time effect and model parameter. In the second experiment, we
    showed that quantifier representations are stable over time as reflected in
    stable model parameters across two experimental sessions. These findings
    challenge semantic theories that assume the truth-conditional equivalence of
    most and more than half and contribute to the representational theory of
    vague concepts. The current study presents a promising approach to study
    semantic representations, which can have a wide application in experimental
    linguistics.
  accessed:
    - year: 2023
      month: 1
      day: 15
  author:
    - family: Ramotowska
      given: Sonia
    - family: Steinert-Threlkeld
      given: Shane
    - family: Maanen
      given: Leendert
      non-dropping-particle: van
    - family: Szymanik
      given: Jakub
  citation-key: ramotowskaUncoveringStructureSemantic2023
  container-title: Cognitive Science
  DOI: 10.1111/cogs.13234
  ISSN: 1551-6709
  issue: '1'
  issued:
    - year: 2023
  language: en
  page: e13234
  source: Wiley Online Library
  title: >-
    Uncovering the Structure of Semantic Representations Using a Computational
    Model of Decision-Making
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13234
  volume: '47'

- id: reymondMSCANDatasetMultilingual2023
  abstract: >-
    Language models achieve remarkable results on a variety of tasks, yet still
    struggle on compositional generalisation benchmarks. The majority of these
    benchmarks evaluate performance in English only, leaving us with the
    question of whether these results generalise to other languages. As an
    initial step to answering this question, we introduce mSCAN, a multilingual
    adaptation of the SCAN dataset. It was produced by a rule-based translation,
    developed in cooperation with native speakers. We then showcase this novel
    dataset on some in-context learning experiments, and GPT3.5 and the
    multilingual large language model BLOOM
  accessed:
    - year: 2023
      month: 12
      day: 14
  author:
    - family: Reymond
      given: Amélie
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: reymondMSCANDatasetMultilingual2023
  container-title: >-
    Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in
    NLP
  editor:
    - family: Hupkes
      given: Dieuwke
    - family: Dankers
      given: Verna
    - family: Batsuren
      given: Khuyagbaatar
    - family: Sinha
      given: Koustuv
    - family: Kazemnejad
      given: Amirhossein
    - family: Christodoulopoulos
      given: Christos
    - family: Cotterell
      given: Ryan
    - family: Bruni
      given: Elia
  event-place: Singapore
  issued:
    - year: 2023
      month: 12
  page: 143–151
  publisher: Association for Computational Linguistics
  publisher-place: Singapore
  source: ACLWeb
  title: 'mSCAN: A Dataset for Multilingual Compositional Generalisation Evaluation'
  title-short: mSCAN
  type: paper-conference
  URL: https://aclanthology.org/2023.genbench-1.11

- id: schlenkerAncestralMeaningsPreludeforthcoming
  abstract: >-
    How did the very first meaning components arise in animals? We argue that
    answers interact in interesting ways with data on current and ancestral
    animal communication systems. Using standard notions of evolutionary
    stability in biology, we develop a simple framework to analyze the emergence
    of three meaning components: individual signals, non-trivial combinations,
    and pragmatic principles of competition among signals. We show that for
    elementary signals to arise, they should have null cost, or be understood
    from the start. While this conclusion dovetails with the traditional idea
    that signals often originate in cues, i.e. informative by-products of
    non-communicative processes, the two scenarios (null cost vs. understanding
    from the start) can be distinguished in case studies involving ancestral
    meaning reconstruction. For non-trivial combinations of the form CC' (such
    as pyow-hack sequences in putty-nosed monkeys and ABC-D sequences in
    Japanese tits), we show that their emergence is heavily constrained because
    they should initially give rise to some miscommunication, as CC' could also
    be understood as the (trivial) combination of separate utterances C and C'.
    Finally, we investigate the evolution of two pragmatic principles that were
    posited in recent animal linguistics: the Informativity Principle and the
    Urgency Principle. We argue that both have a clear evolutionary path,
    especially if they start appearing in production, and then in comprehension.
    Overall, recent work in animal linguistics can be fruitfully combined with
    simple principles of evolutionary stability and with ancestral signal
    reconstruction to address in a precise fashion questions about the very
    first meaning operations in nature.
  author:
    - family: Schlenker
      given: Philippe
    - family: Pawlowitsch
      given: Christina
    - family: Arnal
      given: Luc
    - family: Chatain
      given: Keny
    - family: Ravaux
      given: Lucie
    - family: Ryder
      given: Robin
    - family: Salis
      given: Ambre
    - family: Steinert-Threlkeld
      given: Shane
    - family: Wang
      given: Léo
    - family: Chemla
      given: Emmanuel
  citation-key: schlenkerAncestralMeaningsPreludeforthcoming
  container-title: Linguistics and Philosophy
  issued:
    - literal: forthcoming
  note: 'LingBuzz Published In: To appear in Linguistics & Philosophy'
  source: LingBuzz
  title: 'Ancestral Meanings: A Prelude to Evolutionary Animal Linguistics'
  title-short: Ancestral Meanings
  type: article-journal
  URL: https://ling.auf.net/lingbuzz/008203

- id: schlenkerAnthropocentrismComparativeCognition2022
  accessed:
    - year: 2022
      month: 12
      day: 15
  author:
    - family: Schlenker
      given: Philippe
    - family: Coye
      given: Camille
    - family: Steinert-Threlkeld
      given: Shane
    - family: Klinedinst
      given: Nathan
    - family: Chemla
      given: Emmanuel
  citation-key: schlenkerAnthropocentrismComparativeCognition2022
  container-title: Cognitive Science
  DOI: 10.1111/cogs.13220
  ISSN: 1551-6709
  issue: '12'
  issued:
    - year: 2022
  language: en
  page: e13220
  source: Wiley Online Library
  title: >-
    Beyond Anthropocentrism in Comparative Cognition: Recentering Animal
    Linguistics
  title-short: Beyond Anthropocentrism in Comparative Cognition
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13220
  volume: '46'

- id: schlenkerAntiBabelThreeDegrees2025
  abstract: >-
    While recent “animal linguistics” treats call form as arbitrary, various
    results suggest that some animals use a biological code to understand the
    calls of unrelated/unfamiliar species. To clarify matters, we distinguish
    among three degrees of interspecies comprehension. In the first (“Understand
    thy neighbor”), a species understands the calls of a neighboring species
    through exposure. In the second (“call convergence”), it understands the
    calls of an unrelated/unfamiliar species through evolutionary convergence
    and resemblance to familiar calls. In the third degree (“featural
    interpretation”), it uses a rule associating a meaning to a specific
    acoustic feature—hence a new road to (featural) compositionality.
  accessed:
    - year: 2025
      month: 7
      day: 9
  author:
    - family: Schlenker
      given: Philippe
    - family: Coye
      given: Camille
    - family: Salis
      given: Ambre
    - family: Steinert-Threlkeld
      given: Shane
    - family: Ravaux
      given: Lucie
    - family: Chemla
      given: Emmanuel
  citation-key: schlenkerAntiBabelThreeDegrees2025
  container-title: Mind & Language
  DOI: 10.1111/mila.12529
  ISSN: 1468-0017
  issued:
    - year: 2025
  language: en
  license: © 2024 John Wiley & Sons Ltd.
  source: Wiley Online Library
  title: 'Anti-Babel: Three degrees of interspecies comprehension'
  title-short: Anti-Babel
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/mila.12529

- id: schlenkerMinimalCompositionalityBird2024
  abstract: >-
    It was argued in a series of experimental studies that Japanese tits (Parus
    minor) have an ABC call that has an alert function, a D call that has a
    recruitment function, and an ABC-D call that is compositionally derived from
    ABC and D, and has a mobbing function. A key conclusion was that ABC-D
    differs from the combination of separate utterances of ABC and of D (e.g. as
    played by distinct but close loudspeakers). While the logic of the argument
    is arguably sound, no explicit rule has been proposed to derive the meaning
    of ABC-D from that of its parts. We compare two analyses. One posits a
    limited instance of semantic compositionality ('Minimal Compositionality');
    the other does without compositionality, but with a more sophisticated
    pragmatics ('Bird Implicatures'). Minimal Compositionality takes the
    composition of ABC and D to deviate only minimally from what would be found
    with two independent utterances: ABC means that 'there is something that
    licenses an alert', D means that 'there is something that licenses
    recruitment', and ABC-D means that 'there is something that licenses both an
    alert and recruitment'. By contrast, ABC and D as independent utterances
    yield something weaker, namely: 'there is something that licenses an alert,
    and there is something that licenses recruitment', without any 'binding'
    across the two utterances. The second theory, Bird Implicatures, only
    requires that ABC-D should be more informative than ABC, and/or than D. It
    builds on the idea, proposed for several monkey species, that a less
    informative call competes with a more informative one (‘Informativity
    Principle’): when produced alone, ABC and D trigger an inference that ABC-D
    is false. We explain how both Minimal Compositionality and Bird Implicatures
    could have evolved, and we compare the predictions of the two theories.
    Finally, we extend the discussion to some chimpanzee and meerkat sequences
    that might raise related theoretical problems.
  accessed:
    - year: 2024
      month: 2
      day: 26
  author:
    - family: Schlenker
      given: Philippe
    - family: Salis
      given: Ambre
    - family: Leroux
      given: Maël
    - family: Coye
      given: Camille
    - family: Rizzi
      given: Luigi
    - family: Steinert-Threlkeld
      given: Shane
    - family: Chemla
      given: Emmanuel
  citation-key: schlenkerMinimalCompositionalityBird2024
  container-title: Biological Reviews
  DOI: 10.1111/brv.13068
  issued:
    - year: 2024
  note: 'LingBuzz Published In: To appear in Biological Reviews'
  source: LingBuzz
  title: >-
    Minimal Compositionality versus Bird Implicatures:  Two Theories of ABC-D
    Sequences in Japanese Tits
  title-short: Minimal Compositionality versus Bird Implicatures
  type: article-journal
  URL: https://ling.auf.net/lingbuzz/007422

- id: shapiroIconicArtificialLanguage2023
  author:
    - family: Shapiro
      given: Naomi Tachikawa
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: shapiroIconicArtificialLanguage2023
  container-title: Proceedings of the Annual Meeting of the Cognitive Science Society
  issued:
    - year: 2023
  title: >-
    Iconic Artificial Language Learning: A Conceptual Replication with English
    Speakers
  type: paper-conference
  URL: https://escholarship.org/uc/item/7b66s7c6
  volume: '45'

- id: shapiroIconicArtificialLanguage2024
  abstract: >-
    The present study examines the feasibility of conducting iconic artificial
    language learning (ALL) experiments in a fieldwork setting. We taught the
    pictographic language from Shapiro and Steinert-Threlkeld (2023) to speakers
    of San Martín Peras Mixtec in Oaxaca, Mexico. In a qualitative analysis, we
    explore whether these speakers display similar word-ordering behaviors to
    those observed among other populations, while developing insights for future
    ALL field experiments. We show that iconic ALL offers a promising path
    forward for including understudied communities in the cognitive sciences.
  author:
    - family: Shapiro
      given: Naomi Tachikawa
    - family: Hedding
      given: Andrew
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: shapiroIconicArtificialLanguage2024
  container-title: Proceedings of the Annual Meeting of the Cognitive Science Society
  issued:
    - year: 2024
  title: >-
    Iconic Artificial Language Learning in the Field: An Experiment with San
    Martín Peras Mixtec Speakers
  title-short: Iconic Artificial Language Learning in the Field
  type: paper-conference
  URL: https://escholarship.org/uc/item/9ds5n1qs
  volume: '46'

- id: shapiroMultilabelApproachMorphosyntactic2021
  abstract: >-
    We propose using a multilabel probing task to assess the morphosyntactic
    representations of multilingual word embeddings. This tweak on canonical
    probing makes it easy to explore morphosyntactic representations, both
    holistically and at the level of individual features (e.g., gender, number,
    case), and leads more naturally to the study of how language models handle
    co-occurring features (e.g., agreement phenomena). We demonstrate this task
    with multilingual BERT (Devlin et al., 2018), training probes for seven
    typologically diverse languages: Afrikaans, Croatian, Finnish, Hebrew,
    Korean, Spanish, and Turkish. Through this simple but robust paradigm, we
    verify that multilingual BERT renders many morphosyntactic features
    simultaneously extractable. We further evaluate the probes on six held-out
    languages: Arabic, Chinese, Marathi, Slovenian, Tagalog, and Yoruba. This
    zero-shot style of probing has the added benefit of revealing which
    cross-linguistic properties a language model recognizes as being shared by
    multiple languages.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Shapiro
      given: Naomi Tachikawa
    - family: Paullada
      given: Amandalynne
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: shapiroMultilabelApproachMorphosyntactic2021
  container-title: 'Findings of the Association for Computational Linguistics: EMNLP 2021'
  DOI: 10.18653/v1/2021.findings-emnlp.382
  event-place: Punta Cana, Dominican Republic
  event-title: EMNLP-Findings 2021
  issued:
    - year: 2021
      month: 11
  page: 4486–4524
  publisher: Association for Computational Linguistics
  publisher-place: Punta Cana, Dominican Republic
  source: ACLWeb
  title: A multilabel approach to morphosyntactic probing
  type: paper-conference
  URL: https://aclanthology.org/2021.findings-emnlp.382

- id: Steinert-Threlkeld2014
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: Steinert-Threlkeld2014
  container-title: Proceedings of Information Dynamics in Artificial Societies (IDAS-14)
  editor:
    - family: Lorini
      given: Emiliano
    - family: Perrussel
      given: Laurent
  issued:
    - year: 2014
  title: Learning to Use Function Words in Signaling Games
  type: paper-conference

- id: Steinert-Threlkeld2017
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: Steinert-Threlkeld2017
  issued:
    - year: 2017
  title: Experimenting with Epistemic Free Choice
  type: article-journal

- id: steinert-threlkeldAlternativeRepresentationsFormal2015
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Munneke
      given: Gert-Jan
    - family: Szymanik
      given: Jakub
  citation-key: steinert-threlkeldAlternativeRepresentationsFormal2015
  container-title: Proceedings of the 20th Amsterdam Colloquium
  editor:
    - family: Brochhagen
      given: Thomas
    - family: Roelofsen
      given: Floris
    - family: Thelier
      given: Nadine
  issued:
    - year: 2015
  page: 368-378
  title: 'Alternative Representations in Formal Semantics: A case study of quantifiers'
  type: paper-conference

- id: steinert-threlkeldCommunicationComputationNew2017
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldCommunicationComputationNew2017
  genre: PhD Dissertation
  issued:
    - year: 2017
  publisher: Stanford University
  title: 'Communication and Computation: New Questions About Compositionality'
  type: thesis
  URL: https://eprints.illc.uva.nl/id/document/12050

- id: steinert-threlkeldCompositionalityCompetitionMonkey2016
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldCompositionalityCompetitionMonkey2016
  container-title: Theoretical Linguistics
  DOI: 10.1515/tl-2016-0009
  issue: 1-2
  issued:
    - year: 2016
  page: 159-171
  title: Compositionality and competition in monkey alert calls
  type: article-journal
  volume: '42'

- id: steinert-threlkeldCompositionalSignalingComplex2016
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldCompositionalSignalingComplex2016
  container-title: Journal of Logic, Language and Information
  DOI: 10.1007/s10849-016-9236-9
  issue: '3'
  issued:
    - year: 2016
  page: 379-397
  title: Compositional Signaling in a Complex World
  type: article-journal
  volume: '25'

- id: steinert-threlkeldComputationalExplanationsSemanticinprep
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik, Jakub
      given: ''
  citation-key: steinert-threlkeldComputationalExplanationsSemanticinprep
  collection-title: Cambridge Elements in Semantics and Pragmatics
  issued:
    - literal: in prep
  publisher: Cambridge University Press
  title: Computational Explanations of Semantic Universals
  type: book

- id: steinert-threlkeldDecidabilityIteratedLanguages2014
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldDecidabilityIteratedLanguages2014
  container-title: >-
    Proceedings of Philosophy, Mathematics, Linguistics: Aspects of Interaction
    (PhML2014)
  editor:
    - family: Prosorov
      given: Oleg
  issued:
    - year: 2014
  page: 215-224
  title: On the Decidability of Iterated Languages
  type: paper-conference

- id: steinert-threlkeldEaseLearningExplains2020
  abstract: >-
    Semantic universals are properties of meaning shared by the languages of the
    world. We offer an explanation of the presence of such universals by
    measuring simplicity in terms of ease of learning, showing that expressions
    satisfying universals are simpler than those that do not according to this
    criterion. We measure ease of learning using tools from machine learning and
    analyze universals in a domain of function words (quantifiers) and content
    words (color terms). Our results provide strong evidence that semantic
    universals across both function and content words reflect simplicity as
    measured by ease of learning.
  accessed:
    - year: 2022
      month: 3
      day: 21
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: steinert-threlkeldEaseLearningExplains2020
  container-title: Cognition
  container-title-short: Cognition
  DOI: 10.1016/j.cognition.2019.104076
  ISSN: 0010-0277
  issued:
    - year: 2020
  language: en
  page: '104076'
  source: ScienceDirect
  title: Ease of learning explains semantic universals
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0010027719302495
  volume: '195'

- id: steinert-threlkeldEmergenceNontrivialCompositionality2020
  abstract: >-
    All natural languages exhibit a distinction between content words (nouns,
    verbs, etc.) and function words (determiners, auxiliaries, tenses, etc.).
    Yet surprisingly little has been said about the emergence of this universal
    architectural feature of human language. This article argues that the
    existence of this distinction requires the presence of nontrivial
    compositionality and identifies assumptions that have previously been made
    in the literature that provably guarantee only trivial composition. It then
    presents a signaling game with variable contexts and shows how the
    distinction can emerge via reinforcement learning.
  accessed:
    - year: 2022
      month: 3
      day: 2
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldEmergenceNontrivialCompositionality2020
  container-title: Philosophy of Science
  DOI: 10.1086/710628
  ISSN: 0031-8248
  issue: '5'
  issued:
    - year: 2020
  page: 897-909
  publisher: The University of Chicago Press
  source: journals.uchicago.edu (Atypon)
  title: Toward the Emergence of Nontrivial Compositionality
  type: article-journal
  URL: https://www.journals.uchicago.edu/doi/10.1086/710628
  volume: '87'

- id: steinert-threlkeldEmergentCommunicationFinetuning2022
  abstract: >-
    We describe an approach of using emergent communication to fine-tune large
    pretrained langauge models, with suggestive pilot results for unsupervised
    translation.
  accessed:
    - year: 2022
      month: 5
      day: 12
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Zhou
      given: Xuhui
    - family: Liu
      given: Zeyu
    - family: Downey
      given: C. M.
  citation-key: steinert-threlkeldEmergentCommunicationFinetuning2022
  container-title: Emergent Communication Workshop at ICLR 2022
  event-title: Emergent Communication Workshop at ICLR 2022
  issued:
    - year: 2022
      month: 3
      day: 7
  language: en
  source: openreview.net
  title: Emergent Communication Fine-tuning (EC-FT) for Pretrained Language Models
  type: paper-conference
  URL: https://openreview.net/forum?id=SUqrM7WR7W5

- id: steinert-threlkeldExplainingSemanticTypology2022
  accessed:
    - year: 2022
      month: 3
      day: 21
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldExplainingSemanticTypology2022
  container-title: Trends in Cognitive Sciences
  container-title-short: Trends in Cognitive Sciences
  DOI: 10.1016/j.tics.2022.02.001
  ISSN: 1364-6613, 1879-307X
  issued:
    - year: 2022
  language: English
  PMID: '35248478'
  publisher: Elsevier
  source: www-cell-com.offcampus.lib.washington.edu
  title: Explaining semantic typology, forms and all
  type: article-journal
  URL: http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(22)00037-7

- id: steinert-threlkeldExplanationVeridicalUniformity2020
  abstract: >-
    A semantic universal, which we here dub the Veridical Uniformity Universal,
    has recently been argued to hold of responsive verbs (those that take both
    declarative and interrogative complements). This paper offers a preliminary
    explanation of this universal: verbs satisfying it are easier to learn than
    those that do not. This claim is supported by a computational experiment
    using artificial neural networks, mirroring a recent proposal for explaining
    semantic universals of quantifiers. This preliminary study opens up many
    avenues for future work on explaining semantic universals more generally,
    which are discussed in the conclusion.
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldExplanationVeridicalUniformity2020
  container-title: Journal of Semantics
  DOI: 10.1093/jos/ffz019
  ISSN: 0167-5133
  issue: '1'
  issued:
    - year: 2020
  license: All rights reserved
  page: 129-144
  title: An Explanation of the Veridical Uniformity Universal
  type: article-journal
  URL: https://academic.oup.com/jos/article/37/1/129/5683663
  volume: '37'

- id: steinert-threlkeldHowSocialNetworks2021
  abstract: >-
    Scholars have offered multiple theoretical resolutions to explain
    inconsistent findings about the relationship of state repression and
    protests, but this repression-dissent puzzle remains unsolved. We simulate
    the spread of protest on social networks to suggest that the
    repression-dissent puzzle arises from the nature of statistical sampling.
    Even though the paper’s simulations construct repression so it can only
    decrease protest size, the strength of repression sometimes correlates with
    a decrease, increase, or no change in protest size, regardless of the type
    of network or sample size chosen. Moreover, the results are most
    contradictory when the repression rate most closely matches that observed in
    real-world data. These results offer a new framework for understanding state
    and protester behavior and suggest the importance of collecting network data
    when studying protests.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Steinert-Threlkeld
      given: Zachary
  citation-key: steinert-threlkeldHowSocialNetworks2021
  container-title: PLOS ONE
  container-title-short: PLOS ONE
  DOI: 10.1371/journal.pone.0250784
  ISSN: 1932-6203
  issue: '5'
  issued:
    - year: 2021
  language: en
  page: e0250784
  publisher: Public Library of Science
  source: PLoS Journals
  title: How social networks affect the repression-dissent puzzle
  type: article-journal
  URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0250784
  volume: '16'

- id: steinert-threlkeldIteratingSemanticAutomata2013
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Icard III.
      given: Thomas F.
  citation-key: steinert-threlkeldIteratingSemanticAutomata2013
  container-title: Linguistics and Philosophy
  DOI: 10.1007/s10988-013-9132-6
  issue: '2'
  issued:
    - year: 2013
  page: 151-173
  title: Iterating semantic automata
  type: article-journal
  volume: '36'

- id: steinert-threlkeldLambdaCalculi2011
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldLambdaCalculi2011
  container-title: The Internet Encyclopedia of Philosophy
  issued:
    - year: 2011
  title: Lambda Calculi
  type: article-journal
  URL: http://www.iep.utm.edu/lambda-calculi/

- id: steinert-threlkeldLearnabilitySemanticUniversals2019
  abstract: >-
    One of the great successes of the application of generalized quantifiers to
    natural language has been the ability to formulate robust semantic
    universals. When such a universal is attested, the question arises as to the
    source of the universal. In this paper, we explore the hypothesis that many
    semantic universals arise because expressions satisfying the universal are
    easier to learn than those that do not. While the idea that learnability
    explains universals is not new, explicit accounts of learning that can make
    good on this hypothesis are few and far between. We develop a model of
    learning — back-propogation through a recurrent neural network — which can
    make good on this promise. In particular, we discuss the universals of
    monotonicity, quantity, and conservativity and perform computational
    experiments of training such a network to learn to verify quantifiers. Our
    results are able to explain monotonicity and quantity quite well. We suggest
    that conservativity may have a different source than the other universals.
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: steinert-threlkeldLearnabilitySemanticUniversals2019
  container-title: Semantics & Pragmatics
  DOI: 10.3765/sp.12.4
  issue: '4'
  issued:
    - year: 2019
  license: All rights reserved
  title: Learnability and Semantic Universals
  type: article-journal
  volume: '12'

- id: steinert-threlkeldOntologicalLabelsAutomated2011
  abstract: >-
    Semantics based Analysis Natural language processing Image and video
    analysis Audio and speech analysis Data and web mining Behavior of software,
    services and networks Security Semantic Integration Metadata and other
    description languages Ontology integration Interoperability and service
    integration Applications using Semantics Search engines and question
    answering Semantic web services Content-based multimedia retrieval and
    editing Context-aware networks of sensors, devices and applications Machine
    translation Creative art description Medicine and biology Semantic
    programming languages and software engineering System design and synthesis
    GIS Semantic Interfaces Natural language interfaces Multimodal interfaces
    Human centered computing. © 2011 IEEE.
  author:
    - family: Steinert-Threlkeld
      given: S.
    - family: Ardekani
      given: S.
    - family: Mejinoz
      given: J.L.V.
    - family: Detwilerz
      given: L.T.
    - family: Brinkleyz
      given: J.F.
    - family: Halle
      given: M.
    - family: Kikinis
      given: R.
    - family: Winslowy
      given: R.L.
    - family: Miller
      given: M.I.
    - family: Ratnanather
      given: J.T.
  citation-key: steinert-threlkeldOntologicalLabelsAutomated2011
  container-title: >-
    Proceedings - 5th IEEE International Conference on Semantic Computing, ICSC
    2011
  DOI: 10.1109/ICSC.2011.99
  ISBN: 978-0-7695-4492-2
  issued:
    - year: 2011
  title: Ontological labels for automated location of left ventricular remodeling
  type: paper-conference

- id: steinert-threlkeldOntologicalLabelsAutomated2012
  abstract: >-
    A method for automated location of shape differences in diseased anatomical
    structures via high resolution biomedical atlases annotated with labels from
    formal ontologies is described. In particular, a high resolution magnetic
    resonance image of the myocardium of the human left ventricle was segmented
    and annotated with structural terms from an extracted subset of the
    Foundational Model of Anatomy ontology. The atlas was registered to the end
    systole template of a previous study of left ventricular remodeling in
    cardiomyopathy using a diffeomorphic registration algorithm. The previous
    study used thresholding and visual inspection to locate a region of
    statistical significance which distinguished patients with ischemic
    cardiomyopathy from those with nonischemic cardiomyopathy. Using semantic
    technologies and the deformed annotated atlas, this location was more
    precisely found. Although this study used only a cardiac atlas, it provides
    a proof-of-concept that ontologically labeled biomedical atlases of any
    anatomical structure can be used to automate location-based inferences. ©
    2012 Elsevier Inc..
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Ardekani
      given: Siamak
    - family: Mejino
      given: J.L.V. Jose LV
      suffix: ''
    - family: Detwiler
      given: Landon T L.T.
    - family: Brinkley
      given: J.F. James F
    - family: Halle
      given: Michael
    - family: Kikinis
      given: Ron
    - family: Winslow
      given: Raimond L R.L.
    - family: Miller
      given: M.I. Michael I
    - family: Ratnanather
      given: J.T. Tilak
  citation-key: steinert-threlkeldOntologicalLabelsAutomated2012
  container-title: Journal of Biomedical Informatics
  DOI: 10.1016/j.jbi.2012.02.013
  ISSN: '15320464'
  issue: '3'
  issued:
    - year: 2012
  page: 522-527
  title: Ontological Labels for Automated Location of Anatomical Shape Differences
  type: article-journal
  URL: http://dx.doi.org/10.1016/j.jbi.2012.02.013
  volume: '45'

- id: steinert-threlkeldOpenStandardsWebBased2009
  abstract: >-
    Interactive math tutorials, often called mathlets, are designed to provide a
    more visceral learning experience than traditional textbook methods and to
    enhance intuitive understanding of complex ideas by allowing users to alter
    parameters that influence visual scenes. We describe methods for creating
    such tutorials using the HTML5 canvas element. First, we discuss some
    motivations for writing such mathlets, then walk-through the process of
    creating a mathlet with canvas. Then, we compare canvas to alternatives,
    explaining our decision to use it, and provide links to other demonstrations
    and resources. Copyright 2012, All Rights Reserved, The Mathematical
    Association of America.
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Tilak Ratnanather
      given: J.
    - family: Ratnanather
      given: Tilak
  citation-key: steinert-threlkeldOpenStandardsWebBased2009
  container-title: 'Loci: Developers'
  DOI: 10.4169/loci003340
  ISSN: '19419198'
  issued:
    - year: 2009
  title: >-
    Open Standards, Web-Based Mathlets: Making Interactive Tutorials Using the
    HTML5 canvas Element
  type: article-journal
  URL: http://mathdl.maa.org/mathDL/55/?pa=content&sa=viewDocument&nodeId=3340
  volume: '1'

- id: steinert-threlkeldPayingAttentionFunction2018
  abstract: >-
    All natural languages exhibit a distinction between content words (like
    nouns and adjectives) and function words (like determiners, auxiliaries,
    prepositions). Yet surprisingly little has been said about the emergence of
    this universal architectural feature of natural languages. Why have human
    languages evolved to exhibit this division of labor between content and
    function words? How could such a distinction have emerged in the first
    place? This paper takes steps towards answering these questions by showing
    how the distinction can emerge through reinforcement learning in agents
    playing a signaling game across contexts which contain multiple objects that
    possess multiple perceptually salient gradable properties.
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldPayingAttentionFunction2018
  container-title: Emergent Communication Workshop @ NeurIPS 2018
  issued:
    - year: 2018
  title: Paying Attention to Function Words
  type: paper-conference
  URL: http://arxiv.org/abs/1909.11060

- id: steinert-threlkeldPropertiesIteratedLanguages2016
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldPropertiesIteratedLanguages2016
  container-title: Journal of Logic, Language and Information
  DOI: 10.1007/s10849-016-9239-6
  issue: '2'
  issued:
    - year: 2016
  page: 191-213
  publisher: Springer Netherlands
  title: Some Properties of Iterated Languages
  type: article-journal
  volume: '25'

- id: steinert-threlkeldQuantifiersNaturalLanguage2019
  abstract: >-
    While the languages of the world vary greatly, linguists have discovered
    many restrictions on possible variation. Semantic universals are
    restrictions on the range of variation in meaning across languages.
    Recently, in several domains-e.g. kinship terms, color terms-such universals
    have been argued to arise from a trade-off between simplicity and
    informativeness. In this paper, we apply this method to a prominent domain
    of functions words, showing that the quantifiers in natural language also
    appear to be optimized for this trade-off. We do this by using an
    evolutionary algorithm to estimate the optimal languages, systematically
    manipulating the degree of naturalness of languages, and showing that
    languages become closer to optimal as they become more natural. Our results
    suggest that very general communicative and cognitive pressures may shape
    the lexica of natural languages across both content and function words.
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldQuantifiersNaturalLanguage2019
  container-title: Proceedings of the 22nd Amsterdam Colloquium
  editor:
    - family: Schlöder
      given: Julian J
    - family: McHugh
      given: Dean
    - family: Roelofsen
      given: Floris
  issued:
    - year: 2019
  license: All rights reserved
  page: 513-522
  title: >-
    Quantifiers in natural language optimize the simplicity/informativeness
    trade-off
  type: paper-conference

- id: steinert-threlkeldQuantifiersNaturalLanguage2021
  abstract: >-
    While the languages of the world vary greatly, they exhibit systematic
    patterns, as well. Semantic universals are restrictions on the variation in
    meaning exhibit cross-linguistically (e.g., that, in all languages,
    expressions of a certain type can only denote meanings with a certain
    special property). This paper pursues an efficient communication analysis to
    explain the presence of semantic universals in a domain of function words:
    quantifiers. Two experiments measure how well languages do in optimally
    trading off between competing pressures of simplicity and informativeness.
    First, we show that artificial languages which more closely resemble natural
    languages are more optimal. Then, we introduce information-theoretic
    measures of degrees of semantic universals and show that these are not
    correlated with optimality in a random sample of artificial languages. These
    results suggest both that efficient communication shapes semantic typology
    in both content and function word domains, as well as that semantic
    universals may not stand in need of independent explanation. asdf
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldQuantifiersNaturalLanguage2021
  container-title: Entropy
  DOI: 10.3390/e23101335
  issue: '10'
  issued:
    - year: 2021
  license: All rights reserved
  page: '1335'
  title: >-
    Quantifiers in Natural Language: Efficient Communication and Degrees of
    Semantic Universals
  type: article-journal
  volume: '23'

- id: steinert-threlkeldReferentialGeneralCalls2021
  abstract: >-
    In recent years, the methods of formal semantics and pragmatics have been
    fruitfully applied to the analysis of primate communication systems. Most
    analyses therein appeal to a division of labor between semantics and
    pragmatics which has the following three features: (F1) calls are given
    referential meanings (they provide information about the world rather than
    just about an action to be taken), (F2) some calls have a general meaning,
    and (F3) the meanings of calls in context are enriched by competition with
    more informative calls, along the lines of scalar implicatures. In this
    paper, we develop highly simplified models to independently assess the
    conditions under which such features would emerge. After identifying a
    sufficient condition for (F1), we find a range of conditions under which
    (F2) and (F3) are not evolutionarily stable, and discuss the consequences
    for both modeling and empirical work.
  accessed:
    - year: 2022
      month: 3
      day: 21
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Schlenker
      given: Philippe
    - family: Chemla
      given: Emmanuel
  citation-key: steinert-threlkeldReferentialGeneralCalls2021
  container-title: Linguistics and Philosophy
  container-title-short: Linguist and Philos
  DOI: 10.1007/s10988-021-09322-1
  ISSN: 1573-0549
  issue: '6'
  issued:
    - year: 2021
  language: en
  page: 1317-1342
  source: Springer Link
  title: Referential and general calls in primate semantics
  type: article-journal
  URL: https://doi.org/10.1007/s10988-021-09322-1
  volume: '44'

- id: steinert-threlkeldSemanticUniversalModality2023
  author:
    - family: Steinert-Threlkeld
      given: Shane
    - family: Imel
      given: Nathaniel
    - family: Guo
      given: Qingxia
  citation-key: steinert-threlkeldSemanticUniversalModality2023
  container-title: Semantics & Pragmatics
  DOI: 10.3765/sp.16.1
  issue: '1'
  issued:
    - year: 2023
  license: All rights reserved
  title: A Semantic Universal for Modality
  type: article-journal
  URL: https://doi.org/10.3765/sp.16.1
  volume: '16'

- id: steinert-threlkeldUniformDefinabilityAssertability2017
  author:
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: steinert-threlkeldUniformDefinabilityAssertability2017
  container-title: Proceedings of the 21st Amsterdam Colloquium
  editor:
    - family: Cremers
      given: Alexandre
    - family: Gessel
      given: Thom
      dropping-particle: van
    - family: Roelofsen
      given: Floris
  issued:
    - year: 2017
  page: 445-454
  title: Uniform Definability in Assertability Semantics
  type: paper-conference

- id: szymanikAutomataComplexityMultipleQuantifier2013
  author:
    - family: Szymanik
      given: Jakub
    - family: Steinert-Threlkeld
      given: Shane
    - family: Zajenkowski
      given: Marcin
    - family: Icard III.
      given: Thomas F.
  citation-key: szymanikAutomataComplexityMultipleQuantifier2013
  container-title: Proceedings of the 12th International Conference on Cognitive Modeling
  issued:
    - year: 2013
  title: Automata and Complexity in Multiple-Quantifier Sentence Verification
  type: paper-conference

- id: tienBilingualAlignmentTransfers2022
  abstract: >-
    This work presents methods for learning cross-lingual sentence
    representations using paired or unpaired bilingual texts. We hypothesize
    that the cross-lingual alignment strategy is transferable, and therefore a
    model trained to align only two languages can encode multilingually more
    aligned representations. We thus introduce dual-pivot transfer: training on
    one language pair and evaluating on other pairs. To study this theory, we
    design unsupervised models trained on unpaired sentences and single-pair
    supervised models trained on bitexts, both based on the unsupervised
    language model XLM-R with its parameters frozen. The experiments evaluate
    the models as universal sentence encoders on the task of unsupervised bitext
    mining on two datasets, where the unsupervised model reaches the state of
    the art of unsupervised retrieval, and the alternative single-pair
    supervised model approaches the performance of multilingually supervised
    models. The results suggest that bilingual training techniques as proposed
    can be applied to get sentence representations with multilingual alignment.
  accessed:
    - year: 2022
      month: 5
      day: 24
  author:
    - family: Tien
      given: Chih-chan
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: tienBilingualAlignmentTransfers2022
  container-title: >-
    Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)
  event-place: Dublin, Ireland
  event-title: ACL 2022
  issued:
    - year: 2022
      month: 5
  page: 8696–8706
  publisher: Association for Computational Linguistics
  publisher-place: Dublin, Ireland
  source: ACLWeb
  title: >-
    Bilingual alignment transfers to multilingual alignment for unsupervised
    parallel text mining
  type: paper-conference
  URL: https://aclanthology.org/2022.acl-long.595

- id: vandepolQuantifiersSatisfyingSemantic2021
  abstract: >-
    Despite wide variation among natural languages, there are linguistic
    properties thought to be universal to all or almost all natural languages.
    Here, we consider universals at the semantic level, in the domain of
    quantifiers, which are given by the properties of monotonicity, quantity,
    and conservativity. We investigate whether these universals might be
    explained by differences in complexity. We generate a large collection of
    quantifiers, based on a simple yet expressive grammar, and compute both
    their complexities and whether they adhere to these universal properties. We
    find that quantifiers satisfying semantic universals are less complex: they
    have a shorter minimal description length.
  accessed:
    - year: 2022
      month: 3
      day: 27
  author:
    - family: Pol
      given: Iris
      non-dropping-particle: van de
    - family: Lodder
      given: Paul
    - family: Maanen
      given: Leendert
      non-dropping-particle: van
    - family: Steinert-Threlkeld
      given: Shane
    - family: Szymanik
      given: Jakub
  citation-key: vandepolQuantifiersSatisfyingSemantic2021
  container-title: Proceedings of the Annual Meeting of the Cognitive Science Society
  issue: '43'
  issued:
    - year: 2021
  language: en
  source: escholarship.org
  title: Quantifiers satisfying semantic universals are simpler
  type: article-journal
  URL: https://escholarship.org/uc/item/1vm445rp
  volume: '43'

- id: wangEvaluatingTransformersAbility2023
  abstract: >-
    Despite the fact that Transformers perform well in NLP tasks, recent studies
    suggest that self-attention is theoretically limited in learning even some
    regular and context-free languages. These findings motivated us to think
    about their implications in modeling natural language, which is hypothesized
    to be mildly context-sensitive. We test the Transformer's ability to learn
    mildly context-sensitive languages of varying complexities, and find that
    they generalize well to unseen in-distribution data, but their ability to
    extrapolate to longer strings is worse than that of LSTMs. Our analyses show
    that the learned self-attention patterns and representations modeled
    dependency relations and demonstrated counting behavior, which may have
    helped the models solve the languages.
  accessed:
    - year: 2023
      month: 12
      day: 14
  author:
    - family: Wang
      given: Shunjie
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: wangEvaluatingTransformersAbility2023
  container-title: >-
    Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting
    Neural Networks for NLP
  editor:
    - family: Belinkov
      given: Yonatan
    - family: Hao
      given: Sophie
    - family: Jumelet
      given: Jaap
    - family: Kim
      given: Najoung
    - family: McCarthy
      given: Arya
    - family: Mohebbi
      given: Hosein
  event-place: Singapore
  issued:
    - year: 2023
      month: 12
  page: 271–283
  publisher: Association for Computational Linguistics
  publisher-place: Singapore
  source: ACLWeb
  title: Evaluating Transformer's Ability to Learn Mildly Context-Sensitive Languages
  type: paper-conference
  URL: https://aclanthology.org/2023.blackboxnlp-1.21

- id: wangGQGGeneralizedQuantifier2023
  abstract: >-
    We present a new dataset consisting of various quantifier expressions to
    evaluate the generalization abilities of language models. The dataset
    contains 18,360 prompts encompassing diverse quantifiers, forming the basis
    of a new framework for assessing semantic understanding in this domain. We
    test the effectiveness of our dataset using Pythia models, ranging from 410
    million to 6.9 billion, showing that quantifier-based tasks can be
    challenging for current language models. We make our code and data publicly
    available, such that the dataset can be easily extended or updated based on
    different evaluation needs.
  accessed:
    - year: 2023
      month: 12
      day: 14
  author:
    - family: Wang
      given: Leroy Zhifei
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: wangGQGGeneralizedQuantifier2023
  container-title: >-
    Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in
    NLP
  editor:
    - family: Hupkes
      given: Dieuwke
    - family: Dankers
      given: Verna
    - family: Batsuren
      given: Khuyagbaatar
    - family: Sinha
      given: Koustuv
    - family: Kazemnejad
      given: Amirhossein
    - family: Christodoulopoulos
      given: Christos
    - family: Cotterell
      given: Ryan
    - family: Bruni
      given: Elia
  event-place: Singapore
  issued:
    - year: 2023
      month: 12
  page: 185–192
  publisher: Association for Computational Linguistics
  publisher-place: Singapore
  source: ACLWeb
  title: >-
    GQG: Generalized Quantifier Generalization - A Dataset for Evaluating
    Quantifier Semantics Understanding in Language Models
  title-short: GQG
  type: paper-conference
  URL: https://aclanthology.org/2023.genbench-1.15

- id: yiProbingUnderstandingEnglish2022
  abstract: >-
    We investigate the extent to which verb alternation classes, as described by
    Levin (1993), are encoded in the embeddings of Large Pre-trained Language
    Models (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively
    constructed diagnostic classifiers for word and sentence-level prediction
    tasks. We follow and expand upon the experiments of Kann et al. (2019),
    which aim to probe whether static embeddings encode frame-selectional
    properties of verbs. At both the word and sentence level, we find that
    contextual embeddings from PLMs not only outperform non-contextual
    embeddings, but achieve astonishingly high accuracies on tasks across most
    alternation classes. Additionally, we find evidence that the middle-to-upper
    layers of PLMs achieve better performance on average than the lower layers
    across all probing tasks.
  accessed:
    - year: 2022
      month: 10
      day: 12
  author:
    - family: Yi
      given: David K
    - family: Bruno
      given: James V
    - family: Han
      given: Jiayu
    - family: Zukerman
      given: Peter
    - family: Steinert-Threlkeld
      given: Shane
  citation-key: yiProbingUnderstandingEnglish2022
  container-title: >-
    Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting
    Neural Networks for NLP
  issued:
    - year: 2022
  title: >-
    Probing for Understanding of English Verb Classes and Alternations in Large
    Pre-trained Language Models
  type: paper-conference
  URL: https://openreview.net/forum?id=Fmn3YQmUPYZ
...
